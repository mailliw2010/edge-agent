# core/llm_factory.py
from loguru import logger
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_community.chat_models import ChatTongyi
from langchain_openai import ChatOpenAI

from config import settings

def create_llm_client() -> BaseChatModel:
    """
    LLM å®¢æˆ·ç«¯å·¥å‚å‡½æ•°ã€‚

    æ ¹æ® .env æ–‡ä»¶ä¸­çš„é…ç½®ï¼Œåˆ›å»ºå¹¶è¿”å›ç›¸åº”çš„ LLM å®¢æˆ·ç«¯å®ä¾‹ã€‚
    è¿™ä¸ªå‡½æ•°æ˜¯è¿æ¥ä»£ç é€»è¾‘ä¸åŸºç¡€è®¾æ–½é…ç½®çš„æ¡¥æ¢ã€‚

    Returns:
        ä¸€ä¸ªå®ç°äº† LangChain `BaseChatModel` æ¥å£çš„ LLM å®¢æˆ·ç«¯å®ä¾‹ã€‚

    Raises:
        ValueError: å¦‚æœé…ç½®çš„ LLM_PROVIDER ä¸è¢«æ”¯æŒã€‚
    """
    provider = settings.LLM_PROVIDER

    logger.info(f"ğŸ­ Creating LLM client for provider: '{provider}'")

    if provider == "dashscope":
        # ä½¿ç”¨é€šä¹‰åƒé—®
        llm = ChatTongyi(
            model=settings.LLM_MODEL_NAME,
            dashscope_api_key=settings.DASHSCOPE_API_KEY,
        )
        logger.success(f"âœ… ChatTongyi client created successfully (model: {settings.LLM_MODEL_NAME}).")
        return llm

    elif provider == "vllm":
        # ä½¿ç”¨ vLLM æä¾›çš„ OpenAI å…¼å®¹æ¥å£
        # LangChain çš„ ChatOpenAI å®¢æˆ·ç«¯å¯ä»¥è¿æ¥ä»»ä½• OpenAI å…¼å®¹çš„ API
        llm = ChatOpenAI(
            model=settings.LLM_MODEL_NAME,
            openai_api_base=settings.VLLM_API_BASE,
            openai_api_key=settings.VLLM_API_KEY,
            streaming=settings.LLM_STREAMING, # ä»é…ç½®ä¸­è¯»å–æµå¼è®¾ç½®
            temperature=0.7, # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´é»˜è®¤å‚æ•°
        )
        logger.success(
            f"âœ… vLLM (ChatOpenAI) client created successfully "
            f"(model: {settings.LLM_MODEL_NAME}, API Base: {settings.VLLM_API_BASE}, Streaming: {settings.LLM_STREAMING})."
        )
        return llm

    else:
        # è¿™ä¸ªåˆ†æ”¯ç†è®ºä¸Šä¸ä¼šè¢«æ‰§è¡Œï¼Œå› ä¸º settings.py å·²ç»åšäº†æ£€æŸ¥
        logger.error(f"Unsupported LLM provider: '{provider}'")
        raise ValueError(f"ä¸æ”¯æŒçš„ LLM æä¾›å•†: '{provider}'")
