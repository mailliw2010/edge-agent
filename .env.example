# LLM Configuration
# -----------------
# LLM_PROVIDER: 选择要使用的 LLM 后端。
# 可选项: "dashscope", "vllm"
LLM_PROVIDER="dashscope"

# LLM_MODEL_NAME: 要使用的模型名称。
# - 如果使用 "dashscope", 推荐 "qwen-max", "qwen-turbo" 等。
# - 如果使用 "vllm", 这里应填写你通过 vLLM 加载的模型名称。
LLM_MODEL_NAME="qwen-max"

# LLM_STREAMING: 是否启用流式响应。
# 对于某些模型或 vLLM 版本，关闭流式可以提高稳定性。
# 可选项: "True", "False"
LLM_STREAMING="False"

# --- Provider-Specific Settings ---

# 1. Dashscope (通义千问)
# 当 LLM_PROVIDER="dashscope" 时需要
DASHSCOPE_API_KEY="sk-xxxxxxxxxxxxxxxxxxxxxx"

# 2. vLLM (本地 OpenAI 兼容接口)
# 当 LLM_PROVIDER="vllm" 时需要
VLLM_API_BASE="http://localhost:8000/v1"
VLLM_API_KEY="EMPTY" # vLLM 通常不需要 API Key，但客户端库可能需要一个非空值

# Logging Configuration
LOG_FORMAT="text"  # Log format: "text" for development, "json" for production
LOG_LEVEL="INFO"   # Log level: TRACE, DEBUG, INFO, SUCCESS, WARNING, ERROR, CRITICAL
LOG_FILE=""        # Log file path. If empty, logs will be sent to stdout/stderr. Example: "logs/app.log"
